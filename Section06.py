# -*- coding:utf-8 -*-
'''
ä½œè€…ï¼šcy
æ—¥æœŸï¼š2025å¹´11æœˆ07æ—¥
8.6 å¾ªç¯ç¥ç»ç½‘ç»œçš„ç®€æ´å®ç°
'''

import torch
from torch import nn
from torch.nn import functional as F
import Section02
import Section03
import Section05

# ä¸ºä¸€ä¸ªå®Œæ•´çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªRNNModelç±»ã€‚
# æ³¨æ„ï¼Œrnn_layeråªåŒ…å«éšè—çš„å¾ªç¯å±‚ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„è¾“å‡ºå±‚ã€‚
class RNNModel(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
    # è¿™ä¸ªåˆå§‹åŒ–ä»£ç å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªæ„é€ å‡½æ•°
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # å¦‚æœRNNæ˜¯åŒå‘çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰ï¼Œnum_directionsåº”è¯¥æ˜¯2ï¼Œå¦åˆ™åº”è¯¥æ˜¯1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # å…¨è¿æ¥å±‚é¦–å…ˆå°†Yçš„å½¢çŠ¶æ”¹ä¸º(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)
        # å®ƒçš„è¾“å‡ºå½¢çŠ¶æ˜¯(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,è¯è¡¨å¤§å°)ã€‚
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        # ç®€å•RNN (nn.RNN) æˆ– GRU (nn.GRU)ï¼šå®ƒä»¬åªéœ€è¦ä¸€ä¸ªéšçŠ¶æ€ã€‚
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRUä»¥å¼ é‡ä½œä¸ºéšçŠ¶æ€
            return torch.zeros((self.num_directions * self.rnn.num_layers,
                                batch_size, self.num_hiddens),
                               device=device)
        # LSTM (nn.LSTM)ï¼šå®ƒéœ€è¦ä¸¤ä¸ªçŠ¶æ€ï¼ˆéšçŠ¶æ€å’Œç»†èƒçŠ¶æ€ï¼‰ã€‚
        else:
            # nn.LSTMä»¥å…ƒç»„ä½œä¸ºéšçŠ¶æ€
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))

def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦æ‰§è¡Œçš„ä»£ç """
    print("ğŸš€ Section06.py çš„ä¸»å‡½æ•°")
    # 8.6.1 å®šä¹‰æ¨¡å‹
    batch_size, num_steps = 32, 35
    train_iter, vocab = Section03.load_data_time_machine(batch_size, num_steps)
    num_hiddens = 256
    rnn_layer = nn.RNN(len(vocab), num_hiddens)
    # ä½¿ç”¨å¼ é‡æ¥åˆå§‹åŒ–éšçŠ¶æ€ï¼Œå®ƒçš„å½¢çŠ¶æ˜¯ï¼ˆéšè—å±‚æ•°ï¼Œæ‰¹é‡å¤§å°ï¼Œéšè—å•å…ƒæ•°ï¼‰ã€‚
    state = torch.zeros((1, batch_size, num_hiddens))
    print(state.shape)
    # é€šè¿‡ä¸€ä¸ªéšçŠ¶æ€å’Œä¸€ä¸ªè¾“å…¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨æ›´æ–°åçš„éšçŠ¶æ€è®¡ç®—è¾“å‡ºã€‚
    X = torch.rand(size=(num_steps, batch_size, len(vocab)))
    Y, state_new = rnn_layer(X, state)
    print(Y.shape, state_new.shape)

    # 8.6.2 è®­ç»ƒä¸é¢„æµ‹
    # åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼ŒåŸºäºä¸€ä¸ªå…·æœ‰éšæœºæƒé‡çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚
    device = Section05.try_gpu()
    net = RNNModel(rnn_layer, vocab_size=len(vocab))
    net = net.to(device)
    # å¾ˆæ˜æ˜¾ï¼Œè¿™ç§æ¨¡å‹æ ¹æœ¬ä¸èƒ½è¾“å‡ºå¥½çš„ç»“æœã€‚
    print(Section05.predict_ch8('time traveller', 10, net, vocab, device))
    # ä¸ä¸Šä¸€èŠ‚ç›¸æ¯”ï¼Œç”±äºæ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜çº§APIå¯¹ä»£ç è¿›è¡Œäº†æ›´å¤šçš„ä¼˜åŒ–ï¼Œ
    # è¯¥æ¨¡å‹åœ¨è¾ƒçŸ­çš„æ—¶é—´å†…è¾¾åˆ°äº†è¾ƒä½çš„å›°æƒ‘åº¦ã€‚
    num_epochs, lr = 500, 1
    Section05.train_ch8(net, train_iter, vocab, lr, num_epochs, device)

if __name__ == '__main__':
    main()
