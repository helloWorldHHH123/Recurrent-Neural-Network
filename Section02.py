# -*- coding:utf-8 -*-
'''
ä½œè€…ï¼šcy
æ—¥æœŸï¼š2025å¹´11æœˆ06æ—¥

8.2 æ–‡æœ¬é¢„å¤„ç†

è¿™éƒ¨åˆ†ä¸»è¦å®Œæˆè¯å…ƒåˆ—è¡¨ idx_to_tokenï¼ˆæ·»åŠ æ–°è¯å…ƒï¼‰ã€
è¯å…ƒå­—å…¸ token_to_idxï¼ˆæ·»åŠ è¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„ï¼‰çš„æ„å»ºï¼Œå³è¯è¡¨ï¼Œ
æš‚æœªæ¶‰åŠåˆ°è¯å‘é‡

æ–‡æœ¬çš„å¸¸è§é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼š
1. å°†æ–‡æœ¬ä½œä¸ºå­—ç¬¦ä¸²åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
2. å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºè¯å…ƒï¼ˆå¦‚å•è¯å’Œå­—ç¬¦ï¼‰ã€‚
3. å»ºç«‹ä¸€ä¸ªè¯è¡¨ï¼Œå°†æ‹†åˆ†çš„è¯å…ƒæ˜ å°„åˆ°æ•°å­—ç´¢å¼•ã€‚
4. å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•åºåˆ—ï¼Œæ–¹ä¾¿æ¨¡å‹æ“ä½œã€‚
'''

"""
    æ¨¡å—	          ä¸»è¦ç”¨é€”	            åœ¨AIä¸­çš„å…¸å‹åº”ç”¨
collections	    å¢å¼ºçš„æ•°æ®ç»“æ„	           æ•°æ®ç»Ÿè®¡ã€åˆ†ç»„ã€ç¼“å­˜
    re         æ­£åˆ™è¡¨è¾¾å¼ã€æ–‡æœ¬å¤„ç†	   æ•°æ®æ¸…æ´—ã€ç‰¹å¾æå–ã€æ–‡æœ¬é¢„å¤„ç†
"""

import collections
import re
import hashlib
import os
import tarfile
import zipfile
import requests

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'
DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',
                            '090b5e7e70c295757f55df93cb0a180b9691891a')
def download(name, cache_dir=os.path.join('.', 'data')):
    """ä¸‹è½½ä¸€ä¸ªDATA_HUBä¸­çš„æ–‡ä»¶ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶å"""
    assert name in DATA_HUB, f"{name} ä¸å­˜åœ¨äº {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname # å‘½ä¸­ç¼“å­˜
    print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname

def download_extract(name, folder=None):
    """ä¸‹è½½å¹¶è§£å‹zip/taræ–‡ä»¶"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, 'åªæœ‰zip/taræ–‡ä»¶å¯ä»¥è¢«è§£å‹ç¼©'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir

def download_all():
    """ä¸‹è½½DATA_HUBä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    for name in DATA_HUB:
        download(name)

def read_time_machine():
    """å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œçš„åˆ—è¡¨ä¸­"""
    """
    lines = f.readlines()
    è¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œ    
    è¿”å›ï¼šå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€è¡Œæ–‡æœ¬    
    ä¿ç•™è¡Œå°¾çš„æ¢è¡Œç¬¦ \n
    ä¾‹å¦‚ï¼Œæ–‡ä»¶å†…å®¹ï¼š
    The Time Machine
    Chapter 1
    
    The Time Traveller was expounding...
    è¯»å–ç»“æœï¼š
    lines = [
    "The Time Machine\n",
    "Chapter 1\n", 
    "\n",
    "The Time Traveller was expounding...\n"
    ]
    """
    with open(download('time_machine'), 'r') as f:
        lines = f.readlines()
    """
    re.sub('[^A-Za-z]+', ' ', line)
    [^A-Za-z]+ï¼šåŒ¹é…éå­—æ¯å­—ç¬¦
    [^A-Za-z]ï¼šä¸æ˜¯A-Zæˆ–a-zçš„å­—ç¬¦
    +ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿™æ ·çš„å­—ç¬¦
    æ›¿æ¢ä¸ºç©ºæ ¼ï¼šå°†æ‰€æœ‰éå­—æ¯å­—ç¬¦åºåˆ—æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    ä¾‹å¦‚ï¼šè¾“å…¥ï¼š "Hello, world! 123"
         è¾“å‡ºï¼š "Hello  world   "
    å»é™¤é¦–å°¾ç©ºæ ¼ï¼š.strip()
    è½¬ä¸ºå°å†™ï¼š.lower()
    """
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]


# 8.2.2 è¯å…ƒåŒ–
# ä¸‹é¢çš„tokenizeå‡½æ•°å°†æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆlinesï¼‰ä½œä¸ºè¾“å…¥ï¼Œ
# åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ˆå¦‚ä¸€æ¡æ–‡æœ¬è¡Œï¼‰ã€‚
# æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨ï¼Œè¯å…ƒï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚
# æœ€åï¼Œè¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰ã€‚
def tokenize(lines, token='word'):
    """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ"""
    if token == 'word':
        # split() æ–¹æ³•é»˜è®¤æŒ‰ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰åˆ†å‰²å­—ç¬¦ä¸²ï¼š
        # ç”¨äºå¯¹æ–‡æœ¬ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œåˆ†è¯æ“ä½œ
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

# 8.2.3 è¯è¡¨
# è¯å…ƒçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ¨¡å‹éœ€è¦çš„è¾“å…¥æ˜¯æ•°å­—ï¼Œå› æ­¤è¿™ç§ç±»å‹ä¸æ–¹ä¾¿æ¨¡å‹ä½¿ç”¨ã€‚
# ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œ
# ç”¨æ¥å°†å­—ç¬¦ä¸²ç±»å‹çš„è¯å…ƒæ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­ã€‚
# è¯­æ–™åº“ä¸­ä¸å­˜åœ¨æˆ–å·²åˆ é™¤çš„ä»»ä½•è¯å…ƒéƒ½å°†æ˜ å°„åˆ°ä¸€ä¸ªç‰¹å®šçš„æœªçŸ¥è¯å…ƒâ€œ<unk>â€ã€‚
# å¡«å……è¯å…ƒï¼ˆâ€œ<pad>â€ï¼‰ï¼›åºåˆ—å¼€å§‹è¯å…ƒï¼ˆâ€œ<bos>â€ï¼‰ï¼›åºåˆ—ç»“æŸè¯å…ƒï¼ˆâ€œ<eos>â€ï¼‰ã€‚
"""
è¿™æ®µä»£ç æ­£æ˜¯åœ¨æ„å»ºè¯è¡¨ï¼ˆVocabularyï¼‰
è¯è¡¨å°±æ˜¯æ–‡æœ¬ä¸­æ‰€æœ‰å”¯ä¸€å•è¯ï¼ˆè¯å…ƒï¼‰çš„é›†åˆï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—ç´¢å¼•ã€‚
å…¶æœ¬è´¨æ˜¯ä¸€ä¸ªæ˜ å°„ç³»ç»Ÿï¼š
å•è¯ï¼ˆäººç±»å¯è¯»ï¼‰ â‡„ ç´¢å¼•ï¼ˆæœºå™¨å¯å¤„ç†ï¼‰
"""
class Vocab:
    """æ–‡æœ¬è¯è¡¨"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        """
        counter.items(): å°†å­—å…¸è½¬æ¢ä¸ºå…ƒç»„ï¼Œä¾‹å¦‚ï¼š
        counter = {'the': 3000, 'and': 1500, 'of': 1200, 'machine': 50}
        counter.items() = [('the', 3000), ('and', 1500), ('of', 1200), ('machine', 50)]
        
        key=lambda x: x[1]ï¼šæŒ‰å…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆè¯é¢‘ï¼‰æ’åº
        reverse=Trueï¼šé™åºæ’åˆ—ï¼ˆä»å¤§åˆ°å°ï¼‰
        
        x æ˜¯ counter.items() è¿”å›çš„æ¯ä¸ªå…ƒç»„ã€‚
        """
        # _token_freqs ä¸æ˜¯å­—å…¸ï¼Œè€Œæ˜¯ä¸€ä¸ªæ’åºåçš„å…ƒç»„åˆ—è¡¨ã€‚
        # å°†è¯é¢‘ç»Ÿè®¡ç»“æœæŒ‰é¢‘ç‡ä»é«˜åˆ°ä½æ’åº
        # è¯é¢‘ä¿¡æ¯å­˜å‚¨åœ¨ _token_freqs ä¸­
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],reverse=True)
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå­˜å‚¨æ‰€æœ‰è¯å…ƒ
        # å¦‚ï¼Œ['<unk>', '<pad>', '<bos>', '<eos>']
        self.idx_to_token = ['<unk>'] + reserved_tokens
        # ç»™æœªçŸ¥ã€ä¿ç•™çš„è¯å…ƒåŠ ä¸Šç´¢å¼•
        # ç´¢å¼•ä¿¡æ¯å­˜å‚¨åœ¨ token_to_idx ä¸­ï¼Œå­—å…¸{è¯å…ƒï¼šç´¢å¼•}
        # å¦‚ï¼Œ{'<unk>': 0, 'a': 1, 'b': 2, 'c': 3}
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        # æ›´æ–°è¯å…ƒåˆ—è¡¨ã€è¯å…ƒå­—å…¸
        # æ›´æ–°è¯å…ƒåˆ—è¡¨ idx_to_tokenï¼ˆæ·»åŠ æ–°è¯å…ƒï¼‰
        # æ›´æ–°è¯å…ƒå­—å…¸ token_to_idxï¼ˆæ·»åŠ è¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„ï¼‰
        for token, freq in self._token_freqs:
            # ä¸¢å¼ƒå‡ºç°é¢‘ç‡è¾ƒä½çš„è¯å…ƒ
            if freq < min_freq:
                break
            if token not in self.token_to_idx:   # é¿å…é‡å¤æ·»åŠ 
                self.idx_to_token.append(token)
                # æ·»åŠ ç´¢å¼•
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    # __getitem__ æ˜¯ Python çš„é­”æ³•æ–¹æ³•ï¼Œå½“ä½ ä½¿ç”¨ obj[key] è¯­æ³•æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨ã€‚
    # è¿”å›ç´¢å¼•è€Œä¸æ˜¯è¯é¢‘
    """
     ç´¢å¼• | å•è¯  | è¯å‘é‡ï¼ˆ128ç»´ï¼‰
    -----|-------|-------------------
    0    | <unk> | [0.1, 0.2, ..., 0.8]
    1    | <pad> | [0.0, 0.0, ..., 0.0]
    ...
    4    | the   | [0.3, -0.1, ..., 0.5]  â† ç´¢å¼•4æŒ‡å‘è¿™ä¸ªè¯å‘é‡
    5    | cat   | [0.7, 0.2, ..., -0.3]  â† ç´¢å¼•5æŒ‡å‘è¿™ä¸ªè¯å‘é‡
    ...
    """
    def __getitem__(self, tokens):
        """
        å¤„ç†å•ä¸ªå•è¯çš„æƒ…å†µ,
        å¦‚æœ tokens ä¸æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼ˆå³å•ä¸ªå­—ç¬¦ä¸²ï¼‰
        åœ¨ token_to_idx å­—å…¸ä¸­æŸ¥æ‰¾è¯¥å•è¯çš„ç´¢å¼•
        å¦‚æœå•è¯ä¸å­˜åœ¨ï¼Œè¿”å› self.unkï¼ˆé€šå¸¸æ˜¯ 0ï¼Œå¯¹åº” <unk>ï¼‰
        """
        if not isinstance(tokens, (list, tuple)):
            # get æ˜¯ Python å­—å…¸çš„ä¸€ä¸ªå†…ç½®æ–¹æ³•ï¼Œç”¨äºå®‰å…¨åœ°è·å–å­—å…¸ä¸­çš„å€¼ã€‚
            # ç”¨æ³•ï¼šå­—å…¸.get(é”®, é»˜è®¤å€¼)
            # å¦‚æœé”®å­˜åœ¨äºå­—å…¸ä¸­ â†’ è¿”å›è¯¥é”®å¯¹åº”çš„å€¼
            # å¦‚æœé”®ä¸å­˜åœ¨äºå­—å…¸ä¸­ â†’ è¿”å›æŒ‡å®šçš„é»˜è®¤å€¼
            return self.token_to_idx.get(tokens, self.unk)
        """
        å¤„ç†å•è¯åˆ—è¡¨çš„æƒ…å†µ,
        å¦‚æœ tokens æ˜¯åˆ—è¡¨æˆ–å…ƒç»„
        å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå•è¯é€’å½’è°ƒç”¨ __getitem__    
        è¿”å›ç´¢å¼•åˆ—è¡¨
        """
        return [self.__getitem__(token) for token in tokens]

    # å°†ç´¢å¼•è½¬æ¢å›å•è¯
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        # self.idx_to_token æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä¸æ˜¯å­—å…¸ï¼
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):
    """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡"""
    # è¿™é‡Œçš„tokensæ˜¯1Dåˆ—è¡¨æˆ–2Dåˆ—è¡¨
    # len(tokens) == 0: æ£€æŸ¥ tokens æ˜¯å¦ä¸ºç©ºåˆ—è¡¨
    # isinstance(tokens[0], list): æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æ˜¯åˆ—è¡¨ç±»å‹
    """
    [token for line in tokens for token in line]æ‰§è¡Œé¡ºåºç­‰ä»·äºï¼š
    result = []
    for line in tokens:      # å¤–å±‚å¾ªç¯å…ˆæ‰§è¡Œ
        for token in line:   # å†…å±‚å¾ªç¯åæ‰§è¡Œ
            result.append(token)
    """
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä¸€ä¸ªåˆ—è¡¨
        tokens = [token for line in tokens for token in line]
    # collections.Counter(tokens) çš„ä½œç”¨: ç»Ÿè®¡åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ çš„å‡ºç°æ¬¡æ•°
    # è¿”å›ä¸€ä¸ª Counter å¯¹è±¡ï¼ˆç±»ä¼¼å­—å…¸ï¼‰
    return collections.Counter(tokens)

# 8.2.4 æ•´åˆæ‰€æœ‰åŠŸèƒ½
def load_corpus_time_machine(max_tokens=-1):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨"""
    # è¯»å–æ–‡æœ¬ä¸ºä¸€ä¸ªåˆ—è¡¨å˜é‡lines
    lines = read_time_machine()
    # å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå­—ç¬¦è¯å…ƒï¼Ÿä¸ºå•¥ä¸ç”¨å•è¯è¯å…ƒï¼Ÿ
    tokens = tokenize(lines, 'char')
    """
    å­—ç¬¦è¯å…ƒ: å°†æ–‡æœ¬åˆ†å‰²æˆå•ä¸ªå­—ç¬¦
    text = "Hello world!"
    # å­—ç¬¦çº§åˆ†è¯
    tokens = ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']
    å•è¯è¯å…ƒ: å°†æ–‡æœ¬åˆ†å‰²æˆå®Œæ•´çš„å•è¯
    text = "Hello world!"
    # å•è¯çº§åˆ†è¯
    tokens = ['Hello', 'world', '!']  # æˆ–è€… ['Hello', 'world!']
    """
    # å°†è¯å…ƒè½¬æ¢ä¸ºè¯è¡¨ï¼Œå³å¸¦ç´¢å¼•çš„å­—ç¬¦åˆ—è¡¨
    vocab = Vocab(tokens)
    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    # å°†åˆ†è¯åçš„æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•åºåˆ—
    # è¿™è¡Œä»£ç å°†åµŒå¥—çš„åˆ†è¯ç»“æœå±•å¹³ä¸ºä¸€ä¸ªé•¿çš„æ•°å­—åºåˆ—ï¼Œå³å°†æ‰€æœ‰æ–‡æœ¬æ•°æ®å¹³é“ºæˆä¸€ä¸ªé•¿çš„ç´¢å¼•åºåˆ—ã€‚
    # æ¯ä¸ªè¯å…ƒéƒ½è¢«æ›¿æ¢ä¸ºå®ƒåœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼Œä¸ºåç»­çš„æ¨¡å‹è®­ç»ƒåšå¥½å‡†å¤‡ã€‚
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    # è¿”å›ç´¢å¼•åˆ—è¡¨ï¼ˆæ•´ä¸ªæ–‡æœ¬çš„å¹³é“ºæ•°å­—åºåˆ—ï¼‰ã€è¯è¡¨å¯¹è±¡ï¼ˆå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„ï¼‰
    # Vocabç±»å¯¹è±¡å†…éƒ¨åŒ…å«ï¼š
    # token_to_idx å­—å…¸ï¼š{'a': 1, 'b': 2, ...}
    # idx_to_token åˆ—è¡¨ï¼š['<unk>', 'a', 'b', ...]
    return corpus, vocab

def main():
    """ä¸»å‡½æ•°ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦æ‰§è¡Œçš„ä»£ç """
    print("ğŸš€ Section02.py çš„ä¸»å‡½æ•°")
    # 8.2.1 è¯»å–æ•°æ®é›†
    # ä»H.G.Wellçš„æ—¶å…‰æœºå™¨99ä¸­åŠ è½½æ–‡æœ¬ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“å°çš„è¯­æ–™åº“ï¼Œæœ‰30000å¤šä¸ªå•è¯ï¼Œ
    # åªä½†è¶³å¤Ÿå°è¯•ç‰›åˆ€ï¼Œè€Œç°å®ä¸­çš„æ–‡æ¡£é›†åˆå¯èƒ½ä¼šåŒ…å«æ•°åäº¿ä¸ªå•è¯ã€‚
    lines = read_time_machine()
    print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')
    print(lines[0])
    print(lines[10])

    # 8.2.2 è¯å…ƒåŒ–
    # å®é™…ä¸Šå°±æ˜¯å­—ç¬¦ä¸²å¤„ç†
    tokens = tokenize(lines)
    # è¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰ã€‚
    for i in range(11):
        print(tokens[i])

    # 8.2.3 è¯è¡¨
    # é¦–å…ˆä½¿ç”¨æ—¶å…‰æœºå™¨æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“æ¥æ„å»ºè¯è¡¨ï¼Œç„¶åæ‰“å°å‰å‡ ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•ã€‚
    vocab = Vocab(tokens)
    print('*'*164)
    print(list(vocab.token_to_idx.items())[:10])
    # å°†æ¯ä¸€æ¡æ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨
    for i in [0, 10]:
        print('æ–‡æœ¬:', tokens[i])
        print('ç´¢å¼•:', vocab[tokens[i]])

    # 8.2.4 æ•´åˆæ‰€æœ‰åŠŸèƒ½
    corpus, vocab = load_corpus_time_machine()
    print('len(corpus) = ',len(corpus), ', len(vocab) = ', len(vocab))


if __name__ == '__main__':
    main()
